\documentclass[11pt,a4paper]{report}

\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage[parfill]{parskip}

% For proofs
\usepackage{amsthm}
\newtheorem*{remark}{Remark}

\begin{document}
\title{Neural Uncertainty}
\author{Max Cohen}
\maketitle

\chapter{Model definition}
\section{Notations}
We consider the prediction task of a hidden state $(x^1, \cdots x^T)$ given a set of observations $(y^1, \cdots y^T)$.

\section{Model}
We define a $L$ layer RNN followed by a fully connected layer. At time step $t$,

\begin{equation*}
    \left\{
    \begin{aligned}
        x_{t+1}   & = \tanh(W_x h_{t+1}^L + b_x)                                                               \\
        h_{t+1}^l & = \tanh(W_{hy}^l h^{l-1}_{t+1} + W_{hh}^l h^{l}_{t} + b_h^l) \quad \forall 1 \leq l \leq L \\
    \end{aligned}
    \right.
\end{equation*}

with $h_{t}^0 \equiv y_{t} \; \forall t$ and $h_{0}^l \equiv 0 \; \forall 1 \leq l \leq L$ .

Let's consider the weights of the last RNN and fully connected layers as $\Theta \equiv (W_{hy}^L, W_{hh}^L, b_h^L, W_x, b_x)$. We can define a new matrix $h_t$ at each time step corresponding to the concatenation of all RNN layers: $h_t \equiv (h_t^1 \cdots h_t^L)$. We also introduce two sequences of random noises as i.i.d real valued random variables $\epsilon$ and $\eta$. We can now write our model in terms of two functions $f$ and $g$ as:

\begin{equation}
    \left\{
    \begin{aligned}
        x_{t+1} & = f_\Theta(h_t) + \epsilon_t  & \text{observation model} \\
        h_{t+1} & = g_\Theta(h_t, y_t) + \eta_t & \text{state model}       \\
    \end{aligned}
    \right.
\end{equation}

\begin{align*}
    Q(\hat \Theta_p, \Theta) & = \mathop{\mathbb{E}_{\hat \Theta_p}} \left[ \log \; p_{\Theta}(X_{1:T}, h_{1:T}, y_{1:T}) | X_{1:T} \right] \\
                             & \simeq \frac{1}{M} \sum_{i=1}^M  log \; p_{\Theta}(X_{1:T}, h_{1:T}, y_{1:T})
\end{align*}
\end{document}