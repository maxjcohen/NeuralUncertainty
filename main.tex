\documentclass[10pt,a4paper]{report}

\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage[parfill]{parskip}
\usepackage{hyperref}
\usepackage{color}
\usepackage[makeroom]{cancel}
\tolerance=500

\DeclareMathOperator*{\argmin}{\arg\!\min}

\begin{document}
\title{Sequential Neural Monte Carlo}
\author{Max Cohen}

% \maketitle
\tableofcontents

\chapter{Model}
\section{Notations}
We consider the prediction task of a set of observations $(y^0, \cdots y^T)$ given a set of input $(u^0, \cdots u^T)$.
The model can be built on top of a arbitrary encoder model, denoted $h$, that will be trained by gradient descent.
Through this encoder, inputs are mapped to latent vectors:
$$
        h: (u^0, \cdots u^T) \mapsto (\tilde u^0, \cdots \tilde u^T)
$$

\section{Model}
Our model can be viewed as a RNN layer followed by a fully connected layer. At time step $t$,

\begin{equation*}
        \left\{
        \begin{aligned}
                y_{t+1} & = \tanh(W_y x_{t+1}^L + b_y)                        \\
                x_{t+1} & = \tanh(W_{xx} x_{t} + W_{xu} \tilde u_{t+1} + b_x) \\
        \end{aligned}
        \right.
\end{equation*}

with $x_{0} \equiv 0$ .

Let's consider the weights of the last RNN and fully connected layers as $\theta \equiv (W_{xx}, \textcolor{red}{\cancel{W_{xu}}}, b_x, W_y, b_y)$.
We introduce two sequences of random noises as i.i.d real valued random variables $\epsilon$ and $\eta$, with covariance matrices $\Sigma_y$ and $\Sigma_x$.
We can now write our model in terms of two functions $f$ and $g$ as:

\begin{equation}
        \left\{
        \begin{aligned}
                y_t        & = f_\theta(x_t) + \epsilon_t                   & \text{observation model} \\
                x_{t+1}    & = g_\theta(x_{t}, \tilde u_{t+1}) + \eta_{t+1} & \text{state model}       \\
                \tilde u_t & = h(u_t)                                       & \text{command model}
        \end{aligned}
        \right.
        \label{model_definition}
\end{equation}

In the following section, we will focus on maximizing the joint log likelihood:
\begin{align}
        \log \; p_{\theta}(X_{0:T}, y_{0:T}, u_{0:T})
        \label{log_likelihood}
\end{align}

\section{Minimization}
We can start by developing the log likelihood:

\begin{align*}
        \log p_{\theta}(X_{0:T}, y_{0:T}, u_{0:T}) & = \frac{1}{T} \log\left(p_\theta(x_0)p_\theta(y_0 | x_0)\prod_{k=1}^{T} p_{\theta}(x_k | x_{k-1}, u_k) p_{\theta}(y_k | x_k)\right) \\
                                                   & = \frac{1}{T} \log p_\theta(x_0)                                                                                                    \\
                                                   & + \frac{1}{T} \sum_{k=1}^{T} \log \; p_{\theta}(x_k | x_{k-1}, u_k) + \frac{1}{T} \sum_{k=0}^{T} \log \; p_{\theta}(y_k | x_k)      \\
                                                   & = \frac{1}{T} \log p_\theta(x_0) -\frac{1}{2} \log|\Sigma_x| -\frac{1}{2} \log|\Sigma_y| + Cst                                      \\
                                                   & - \frac{1}{2T} \sum_{k=1}^{T}(x_k - g_\theta(x_{k-1}, u_{k}))' \Sigma_x^{-1} (x_k - g_\theta(x_{k-1}, u_{k}))                       \\
                                                   & - \frac{1}{2T} \sum_{k=0}^{T}(y_k - f_\theta(x_k))' \Sigma_y^{-1} (y_k - f_\theta(x_k))                                             \\
\end{align*}

We aim at maximizing \ref{log_likelihood} by gradient descent, by leveraging fisher's identity:
$$
        \nabla \log p_\theta(x_{0:T}, y_{0:T}, u_{0:T}) = \mathbb{E}_\theta \left[ \nabla\log p_\theta(x_{0:T}, y_{0:T}, u_{0:T}) | Y_{0:T} \right]
$$
In order to approximate this expectation, we need to sample from the posterior distribution $p_\theta(x|y)$.
In Section~\ref{sec:smc}, we detail a Sequential Monte Carlo approach to this end.
In Section~\ref{sec:gradient_descent}, we describe the algorithm to train our model through gradient descent.


\section{Sequential Monte Carlo Approach}
\label{sec:smc}

\subsection{Filter}
In order to compute the conditional expectations in the previous expressions, we will iteratively sample trajectories $\xi_{1:T}^i$ associated with weights $\omega^i$ with respect to the density $p_\theta(x | y)$, using a sequential Monte Carlo particle filter.

At time step $k=0$, $(\xi_0^l)_{l=1}^N$ are sampled independently from the first hidden state, and associated with sampling weights proportional to the observation density $q_\theta$:
\begin{align*}
        \xi_0^i    & \sim \mathcal{N}(x_0, \Sigma_x) \\
        \omega_0^i & \sim q_\theta(\xi_0^i)
\end{align*}

At time step $k+1$, we sample indices $I$ of the particles to propagate, based on their previous weights.
After propagation, particles weights are computed following the observation density function:
$$\mathbb{P}(I_{k+1}^i=j) = \omega_k^j \quad \forall 1 \leq j \leq N$$
$$\omega_{k+1}^i \sim q_\theta(\xi_{k+1}^i)$$

\subsection{Smoother}
Using the poor man filter, we get $N$ trajectories:
$$\xi_{1:k+1}^{i} = (\xi_{1:k}^{I_{k+1}^i}, \xi_{k+1}^i)$$

\subsection{Approximation}
We can now approximate this conditional expectation for any measurable bounded function $h$:
\begin{align*}
        \mathbb{E}_\theta \left[ h(x) | y_{1:T} \right] = \sum_{i=1}^N \omega_T^i h(\xi_{0:T}^i)
\end{align*}

\section{Gradient descent}
\label{sec:gradient_descent}

\subsection{Forward pass}
During the forward pass, we generate a set of $N$ particles under the law $p(x|y)$ for fixed values of $\theta$, $\Sigma_x$ and $\Sigma_y$.
We initialize the sequence with a initial hidden state sampled from the noise's distribution.
$$
        x^i_0 \sim \mathcal{N}(0, \Sigma_x)
$$
In order to predict each new time step $k+1$, particles from the previous step are attributed weights $\omega_k^i$ proportionally to the density probability around the targeted value $y_k$.

$$\omega_k^i \sim \exp(-\frac{1}{2}(y_k - f_{\theta_p}(x_k^i))'\Sigma^{-1}_{y, p}(y_k - f_{\theta_p}(x_k^i)))$$
We then select a new population from these particles indexed by $I_{k+1}^i$, based on their weights.
$$\mathbb{P}(I_{k+1}^i=j) = \omega_k^j \quad \forall 1 \leq j \leq N$$
The current hidden state is computed for the selected particles.
$$x^i_{k+1} = g_\theta(x_k^{I_{k+1}^i}, u_{k+1}) + \eta^i_{k+1}$$

\subsection{Loss function}
Considering that we have computed a set of $N$ trajectories $(\xi^i_{0:T}),\;1 \leq i \leq N$, associated with weights $(\omega^i)$, we can approximate the gradient of the log likelihood by computing the gradient of:
\begin{align*}
        \mathbb{J}(\theta) & = \log |\Sigma_x| + \log |\Sigma_y|                                                                                                        \\
                           & + \frac{1}{T}\sum_{k=1}^T \sum_{i=1}^N \omega^i (y_k - f_\theta(\xi_k^i))' \Sigma_y^{-1} (y_k - f_\theta(\xi_k^i))                         \\
                           & + \frac{1}{T}\sum_{k=0}^T \sum_{i=1}^N \omega^i (\xi_k^i - g_\theta(\xi_{k-1}^i, u_k))'\Sigma_x^{-1}(\xi_k^i - g_\theta(\xi_{k-1}^i, u_k))
\end{align*}

\subsection{Backward pass}
During this step, each parameter of the model is updated by gradient descent.

\section{Two-step training}
In this section, we aim at improving the weights of a model already trained in a traditional fashion.

\subsection{Initial training}
\label{sec:pretrain}
Given a dataset of samples $(u_{0:T}^{(i)}, y_{0:T}^{(i)})_{i=1}^m$, we consider a training of our model as defined in \ref{model_definition} without the added noise.
For each sample, we simply compute a prediction $\hat y_{0:T}$ by running the input through each layer, and minimize the discrepancy to the target values by gradient descent.
$$
        \mathbb{J} = \| u_{0:T} - y_{0:T} \|^2
$$
This training results in an initial set of weights $\theta_0$.

\subsection{Finetuning}
Because we trust the initial training has successfully extracted relevant information from the command $u$, and reached satisfactory parameters for the hidden state model, we will only adapt the parameters of the observation model:
$$
        \theta_{finetune} \equiv (\cancel{W_{xx}}, \cancel{b_x}, W_y, b_y)
$$
$\Sigma_x$ is set to a small value and will not be updated during the finetuning.

\chapter{Simulation plan}
\section{Finetuning for noisy dataset}
For this simulation, our objective is to generate a noisy dataset of pairs $(u_{0:T}^{(i)}, y_{0:T}^{(i)})_{i=1}^m$, to obtain an initial set of parameters by traditional gradient descent minimizing a loss criterion, then fine-tune by adding noise to the model and maximizing the log likelihood.
In this section, we assume that $h$ is the identity function, meaning the latent vector is equal to the input:
$$
        \tilde u \equiv u
$$

\subsection{Dataset}
We sample the dataset using the model defined in \ref{model_definition}.
$W_{xi}$ is set to a high value, and $W_{xx}$ to a small value, for the observation to carry more information from the input than the hidden state.
This will prevent the particle filter to simply learn a good posterior.
$\Sigma_x$ and $\Sigma_y$ are set to small values in order to keep the model's dependency on input and hidden state, while still preventing a traditional training from obtaining zero loss.
We use $T=50$ and generate $m=100$ samples.

\subsection{Pre training}
Pre training is implemented as described in Section~\ref{sec:pretrain}.
In our experimentation, we were able to recover the simulation parameters, although the model does not require to be identifiable, as demonstrated in the next simulation plan.
During prediction, the model matches our simulated dataset to some extent.
The general trend were correctly followed, while some of the most extremal points could not be reached, as this simple model doesn't account for model or observation noise.

\subsection{Finetuning}
We now freeze the model's parameters, except for $W_y$, $b_y$ and $\Sigma_y$.
$\Sigma_x$ is set to a small value.
We maximize the log likelihood as previously described in the model definition.
We found $W_y$ and $b_y$ to vary during training, but eventually reached the original values at convergence.
We were also able to recover the original value of $\Sigma_y$.

\subsection{Training from scratch}
As a comparison, we trained the model by maximizing the log likelihood without any prior trainings.
Weights are not frozen.
We found convergence to be much harder to achieve than both previous methods, but we were still able to recover the value of $\Sigma_y$.

\section{Finetuning advanced models}
In this section, we reiterate the experiment of the previous section while considering a model where $h$ is an arbitrary non linear function, such as a neural network.
We use the same dataset.
Similarly, the static model is able to capture trends, without being able to match all points due to observation noise.
The Bayesian model is able to take advantage of the retrain, and recover the observation noise while only slightly straying from the original values of $W_y$ and $b_y$.

\section{Real data}
This section addresses the training protocol on real data.
We follow the same steps as for synthetic data:

\begin{enumerate}
        \item A training step expected to reach a satisfactory set of parameters for the encoder $h$ and the state model $g_\theta$.
        \item A finetuning step, during which only $W_y$, $b_y$ and $\Sigma_y$ are learned via Fisher's identity, for a set value of $\Sigma_x$.
\end{enumerate}

\end{document}
