\documentclass[10pt,a4paper]{report}

\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage[parfill]{parskip}
\usepackage{hyperref}
\usepackage{color}
\usepackage[makeroom]{cancel}
\tolerance=500

\DeclareMathOperator*{\argmin}{\arg\!\min}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\diag}{diag}

\begin{document}
\title{Sequential Neural Monte Carlo}
\author{Max Cohen}

% \maketitle
\tableofcontents

\chapter{Model}
\section{Notations}
We consider the prediction task of a set of observations $(y^0, \cdots y^T)$ given a set of input $(u^0, \cdots u^T)$.
The model can be built on top of a arbitrary encoder model, denoted $h$, that will be trained by gradient descent.
Through this encoder, inputs are mapped to latent vectors:
$$
	h: (u^0, \cdots u^T) \mapsto (\tilde u^0, \cdots \tilde u^T)
$$

\section{Model}
Our model can be viewed as a RNN layer followed by a fully connected layer. At time step $t$,

\begin{equation*}
	\left\{
	\begin{aligned}
		y_{t+1} & = \tanh(W_y x_{t+1}^L + b_y)                        \\
		x_{t+1} & = \tanh(W_{xx} x_{t} + W_{xu} \tilde u_{t+1} + b_x) \\
	\end{aligned}
	\right.
\end{equation*}

with $x_{0} \equiv 0$ .

Let's consider the weights of the last RNN and fully connected layers as $\theta \equiv (W_{xx}, \textcolor{red}{\cancel{W_{xu}}}, b_x, W_y, b_y)$.
We introduce two sequences of random noises as i.i.d real valued random variables $\epsilon$ and $\eta$, with covariance matrices $\Sigma_y$ and $\Sigma_x$.
We can now write our model in terms of two functions $f$ and $g$ as:

\begin{equation}
	\left\{
	\begin{aligned}
		y_t        & = f_\theta(x_t) + \epsilon_t                   & \text{observation model} \\
		x_{t+1}    & = g_\theta(x_{t}, \tilde u_{t+1}) + \eta_{t+1} & \text{state model}       \\
		\tilde u_t & = h(u_t)                                       & \text{command model}
	\end{aligned}
	\right.
	\label{model_definition}
\end{equation}

In the following section, we will focus on maximizing the log likelihood:
\begin{align}
	\log \; p_{\theta}(y_{0:T})
	\label{log_likelihood}
\end{align}

\section{Minimization}
We aim at maximizing \ref{log_likelihood} by gradient descent, by leveraging fisher's identity:
$$
	\nabla \log p_\theta(y_{0:T}) = \mathbb{E}_\theta \left[ \nabla\log p_\theta(x_{0:T}, y_{0:T}, u_{0:T}) | Y_{0:T} \right]
$$
\subsection{Joint log likelihood}
We can start by developing the joint log likelihood:

\begin{align*}
	\log p_{\theta}(X_{0:T}, y_{0:T}, u_{0:T}) & = \frac{1}{T} \log\left(p_\theta(x_0)p_\theta(y_0 | x_0)\prod_{k=1}^{T} p_{\theta}(x_k | x_{k-1}, u_k) p_{\theta}(y_k | x_k)\right) \\
						   & = \frac{1}{T} \log p_\theta(x_0)                                                                                                    \\
						   & + \frac{1}{T} \sum_{k=1}^{T} \log \; p_{\theta}(x_k | x_{k-1}, u_k) + \frac{1}{T} \sum_{k=0}^{T} \log \; p_{\theta}(y_k | x_k)      \\
						   & = \frac{1}{T} \log p_\theta(x_0) -\frac{1}{2} \log|\Sigma_x| -\frac{1}{2} \log|\Sigma_y| + Cst                                      \\
						   & - \frac{1}{2T} \sum_{k=1}^{T}(x_k - g_\theta(x_{k-1}, u_{k}))' \Sigma_x^{-1} (x_k - g_\theta(x_{k-1}, u_{k}))                       \\
						   & - \frac{1}{2T} \sum_{k=0}^{T}(y_k - f_\theta(x_k))' \Sigma_y^{-1} (y_k - f_\theta(x_k))                                             \\
\end{align*}

\subsection{Optimization of $\Sigma_x$}
We already have a explicit expression for $\Sigma_x$, by searching the zeros of the convex function $\Sigma_x \mapsto p_\theta(x_{0:T}, y_{0:T}, u_{0:T})$.
With $\tilde x_k = x_k - g_\theta(x_{k-1}, u_k)$:

\begin{align*}
	\frac{\partial \log p_\theta(x_{0:T}, y_{0:T}, u_{0:T})}{\partial \Sigma_x}     & = -\frac{1}{2} \Sigma_x^{-1} + \frac{1}{2T} \sum_{k=1}^T \Sigma_x^{-1}\tilde x_k \tilde x_k'\Sigma_x^{-1} \\
	\frac{\partial \log p_\theta(X_{1:T}, y_{1:T}, u_{1:T})}{\partial \Sigma_x} = 0 & \implies \Sigma_x^{-1} = \frac{1}{T}\sum_{k=1}^T \Sigma_x^{-1}\tilde x_k \tilde x_k'\Sigma_x^{-1}         \\
											& \implies \Sigma_x = \frac{1}{T}\sum_{k=1}^T\tilde x_k \tilde x_k'
\end{align*}

We can constrain the covariance matrix to be diagonal in order to simplify computation and interpretation of the model, $\Sigma_x = \diag{\{\sigma_{x, i}^2\}_{i=1}^d}$.
Let $j \in \{1 \cdots d\}$, the derivative of the log likelihood w.r.t $\sigma_{x, j}^2$ can be expressed as:
\begin{align*}
	\frac{\partial}{\partial \sigma_{x, j}^2} & = -\frac{1}{2} \frac{\partial}{\partial \sigma_{x, j}^2} \log|\Sigma_x|                                                                                                                                             \\
						  & - \frac{1}{2T} \frac{\partial}{\partial \sigma_{x, j}^2} \left[ \sum_{k=1}^{T}(x_k - g_\theta(x_{k-1}, u_{k}))' \Sigma_x^{-1} (x_k - g_\theta(x_{k-1}, u_{k})) \right]                                              \\
						  & = - \frac{\partial}{\partial \sigma_{x, j}^2} \left[ \frac{1}{2} \sum_{i=1}^d \log \sigma_{x, i}^2  + \frac{1}{2T} \sum_{k=1}^T \sum_{i=1}^d (x_k - g_\theta(x_{k-1}, u_{k}))_i^2 \frac{1}{\sigma_{x, i}^2} \right] \\
						  & = -\frac{1}{2 \sigma_{x, j}^2} + \frac{1}{2T} \sum_{k=1}^T (x_k - g_\theta(x_{k-1}, u_{k}))_j^2 \frac{1}{\sigma_{x, j}^4}                                                                                           \\
\end{align*}

$$
	\frac{\partial \log p_\theta(X_{1:T}, y_{1:T}, u_{1:T})}{\partial \sigma_{x, j}^2} = 0 \implies \sigma_{x, j}^2 = \frac{1}{T}\sum_{k=1}^T (x_k - g_\theta(x_{k-1}, u_k))_j^2
$$

The minimization of a diagonal covariance matrix matches the diagonal of the minimization of a full matrix:
$$
	\widehat \Sigma_x^{\texttt{diag}} = \diag \left( \widehat \Sigma_x \right)
$$

In order to approximate the expectation, we need to sample from the posterior distribution $p_\theta(x|y)$.
In Section~\ref{sec:smc}, we detail a Sequential Monte Carlo approach to this end.
In Section~\ref{sec:gradient_descent}, we describe the algorithm to train our model through gradient descent.


\section{Sequential Monte Carlo Approach}
\label{sec:smc}

\subsection{Filter}
In order to compute the conditional expectations in the previous expressions, we will iteratively sample trajectories $\xi_{1:T}^i$ associated with weights $\omega^i$ with respect to the density $p_\theta(x | y)$, using a sequential Monte Carlo particle filter.

At time step $k=0$, $(\xi_0^l)_{l=1}^N$ are sampled independently from the first hidden state, and associated with sampling weights proportional to the observation density:
\begin{align*}
	\xi_0^i    & \sim \mathcal{N}(x_0, \Sigma_x) \\
	\omega_0^i & \sim p_\theta(y_0|\xi_0^i)
\end{align*}

At time step $k+1$, we sample indices $I$ of the particles to propagate, based on their previous weights.
After propagation, particles weights are computed following the observation density:
$$\mathbb{P}(I_{k+1}^i=j) = \omega_k^j \quad \forall 1 \leq j \leq N$$

\begin{align*}
	\xi_{k+1}^i    & \sim p_\theta(x_{k+1}|\xi_k^{I_{k+1}^i}) \\
	\omega_{k+1}^i & \sim p_\theta(y_{k+1}|\xi_{k+1}^i)       \\
\end{align*}

\begin{algorithm}
	\KwIn{$\theta, y_{1:T}, u_{1:T}$}
	\KwOut{$\xi_{1:T}^{1:N}$}
	$\xi_0^i \gets \eta^i$\;
	\For{$k \gets 0$ \KwTo $T$}{
		$\omega_k^i \gets \varphi_{y_k, \Sigma_y}(f_\theta(\xi_k^i))$\;
		$I_{k+1}^i \sim \mathbb{P}(I_{k+1}^i=j) = \omega_k^j$\;
		$\xi_{k+1}^i \gets g_\theta(\xi_k^{I_{k+1}^i}, u_{k+1}) + \eta^i$\;
	}
	\caption{Particle filter}
\end{algorithm}

\subsection{Smoother}
Using the poor man filter, we get $N$ trajectories:
$$\xi_{1:k+1}^{i} = (\xi_{1:k}^{I_{k+1}^i}, \xi_{k+1}^i)$$

\subsection{Approximation}
We can now approximate this conditional expectation for any measurable bounded function $h$:
\begin{align*}
	\mathbb{E}_\theta \left[ h(x) | y_{1:T} \right] = \sum_{i=1}^N \omega_T^i h(\xi_{0:T}^i)
\end{align*}

\section{Gradient descent}
\label{sec:gradient_descent}

\subsection{Forward pass}
During the forward pass, we generate a set of $N$ particles under the law $p(x|y)$ for fixed values of $\theta$, $\Sigma_x$ and $\Sigma_y$.

\subsection{Loss function}
Considering that we have computed a set of $N$ trajectories $(\xi^i_{0:T}),\;1 \leq i \leq N$, associated with weights $(\omega^i)$, we can approximate the gradient of the log likelihood by computing the gradient of:
\begin{align*}
	\mathbb{J}(\theta) & = \log |\Sigma_x| + \log |\Sigma_y|                                                                                                        \\
			   & + \frac{1}{T}\sum_{k=1}^T \sum_{i=1}^N \omega^i (y_k - f_\theta(\xi_k^i))' \Sigma_y^{-1} (y_k - f_\theta(\xi_k^i))                         \\
			   & + \frac{1}{T}\sum_{k=0}^T \sum_{i=1}^N \omega^i (\xi_k^i - g_\theta(\xi_{k-1}^i, u_k))'\Sigma_x^{-1}(\xi_k^i - g_\theta(\xi_{k-1}^i, u_k))
\end{align*}

\subsection{Backward pass}
During this step, each parameter of the model is updated by gradient descent.

\subsection{Algorithm}
\begin{algorithm}
	\KwIn{$\theta_0, y_{1:T}, u_{1:T}$}
	\KwOut{$\theta_p$}
	\For{$p \gets 0$ \KwTo $n_{epochs}$}{
		$\xi_{1:T}^{1:N} \gets \texttt{particule\_filter}(y_{1:T})$\;
		$\theta_{p+1} \gets \theta_p - \alpha \nabla \mathbb{J}(\theta_p)$\;
	}
	\caption{Gradient descent}
\end{algorithm}

\section{Two-step training}
In this section, we aim at improving the weights of a model already trained in a traditional fashion.

\subsection{Initial training}
\label{sec:pretrain}
Given a dataset of samples $(u_{0:T}^{(i)}, y_{0:T}^{(i)})_{i=1}^m$, we consider a training of our model as defined in \ref{model_definition} without the added noise.
For each sample, we simply compute a prediction $\hat y_{0:T}$ by running the input through each layer, and minimize the discrepancy to the target values by gradient descent.
$$
	\mathbb{J} = \| u_{0:T} - y_{0:T} \|^2
$$
This training results in an initial set of weights $\theta_0$.

\subsection{Finetuning}
Because we trust the initial training has successfully extracted relevant information from the command $u$, and reached satisfactory parameters for the hidden state model, we will only adapt the parameters of the observation model:
$$
	\theta_{finetune} \equiv (\cancel{W_{xx}}, \cancel{b_x}, W_y, b_y)
$$
$\Sigma_x$ is set to a small value and will not be updated during the finetuning.

\section{Prediction at $t+1$}
Suppose that at time $t$, we have access to past observations $y_{1:t}$, along with current weighted particles trajectories $\{\xi_t^i, \omega_t^i\}_{1 \leq i \leq N}$.
Then,
\begin{align*}
	p(y_{t+1}|y_{1:t}) & = \int p(y_{t+1}, x_{t+1}, x_t|y_{1:t})\mathrm{d}x_{t+1}\mathrm{d}x_t                        \\
			   & = \int p(y_{t+1}|x_{t+1}) p(x_{t+1}|x_t) p(x_t|y_{1:t})\mathrm{d}x_{t+1}\mathrm{d}x_t        \\
			   & \approx \sum_{i=1}^N \omega_t^i \int p(y_{t+1}|x_{t+1}) p(x_{t+1}|\xi_t^i) \mathrm{d}x_{t+1} \\
\end{align*}

In order to approximate the last integral, we sample for each particle $1 \leq i \leq N$ a set of $M$ states under the law $\hat x_{t+1}^{i, j} \sim p(x_{t+1}|\xi_t^i)_{1 \leq j \leq M}$.
We write:
\begin{align}
	p(y_{t+1}|y_{1:t}) \approx \sum_{i=1}^N \omega_t^i \frac{1}{M} \sum_{j=1}^M p(y_{t+1}|\hat x_{t+1}^{i, j})
\end{align}

We want a point estimation of the observation at $t+1$ along with a quantification of the uncertainty, so we compute the mean and variance of $p(y_{t+1}|y_{1:t})$.
Accounting for $p(y_{t+1} | \hat x_{t+1}) \sim \mathcal{N}(f_\theta(\hat x_{t+1}), \Sigma_y)$:
\begin{align*}
	\mu_{t+1|t}      & = \mathbb{E}[y_{t+1}|y_{1:t}] \approx \sum_{i=1}^N \omega_t^i \frac{1}{M} \sum_{j=1}^M \mathbb{E}[y_{t+1}|\hat x_{t+1}^{i, j}] \\
	\sigma_{t+1|t}^2 & = \mathbb{E}[y_{t+1}^2|y_{1:t}] - \mu_{t+1|t}^2                                                                                \\
			 & \approx \sum_{i=1}^N {\omega_t^i} \frac{1}{M} \sum_{j=1}^M \mathbb{E}[y_{t+1}^2|\hat x_{t+1}^{i, j}] - \mu_{t+1|t}^2
\end{align*}

\begin{algorithm}
	\KwIn{$\xi_t^{1:N}, y_{1:t}$}
	\KwOut{$\mu_{t+1}, \sigma_{t+1}$}
	$\hat x_{t+1}^{i, j} = g_\theta(\xi_t^i) + \eta_t^{i, j}$\;
	$\mu_{t+1} = \sum_{i=1}^N \omega_t^i \frac{1}{M} \sum_{j=1}^M f_\theta(\hat \xi_{t+1}^{i, j})$\;
	$\sigma_{t+1}^2 = \Sigma_y + \sum_{i=1}^N \omega_t^i \frac{1}{M} \sum_{j=1}^M f_\theta(\hat \xi_{t+1}^{i, j})^2 - \mu_{t+1|t}^2$\;
	\caption{Prediction at time {t+1}}
\end{algorithm}


\chapter{Simulation plan}
\section{Finetuning for noisy dataset}
For this simulation, our objective is to generate a noisy dataset of pairs $(u_{0:T}^{(i)}, y_{0:T}^{(i)})_{i=1}^m$, to obtain an initial set of parameters by traditional gradient descent minimizing a loss criterion, then fine-tune by adding noise to the model and maximizing the log likelihood.
In this section, we assume that $h$ is the identity function, meaning the latent vector is equal to the input:
$$
	\tilde u \equiv u
$$

\subsection{Dataset}
We sample the dataset using the model defined in \ref{model_definition}.
$W_{xi}$ is set to a high value, and $W_{xx}$ to a small value, for the observation to carry more information from the input than the hidden state.
This will prevent the particle filter to simply learn a good posterior.
$\Sigma_x$ and $\Sigma_y$ are set to small values in order to keep the model's dependency on input and hidden state, while still preventing a traditional training from obtaining zero loss.
We use $T=50$ and generate $m=100$ samples.

\subsection{Pre training}
Pre training is implemented as described in Section~\ref{sec:pretrain}.
In our experimentation, we were able to recover the simulation parameters, although the model does not require to be identifiable, as demonstrated in the next simulation plan.
During prediction, the model matches our simulated dataset to some extent.
The general trend were correctly followed, while some of the most extremal points could not be reached, as this simple model doesn't account for model or observation noise.

\subsection{Finetuning}
We now freeze the model's parameters, except for $W_y$, $b_y$ and $\Sigma_y$.
$\Sigma_x$ is set to a small value.
We maximize the log likelihood as previously described in the model definition.
We found $W_y$ and $b_y$ to vary during training, but eventually reached the original values at convergence.
We were also able to recover the original value of $\Sigma_y$.

\subsection{Training from scratch}
As a comparison, we trained the model by maximizing the log likelihood without any prior trainings.
Weights are not frozen.
We found convergence to be much harder to achieve than both previous methods, but we were still able to recover the value of $\Sigma_y$.

\section{Finetuning advanced models}
In this section, we reiterate the experiment of the previous section while considering a model where $h$ is an arbitrary non linear function, such as a neural network.
We use the same dataset.
Similarly, the static model is able to capture trends, without being able to match all points due to observation noise.
The Bayesian model is able to take advantage of the retrain, and recover the observation noise while only slightly straying from the original values of $W_y$ and $b_y$.

\section{Real data}
This section addresses the training protocol on real data.
We follow the same steps as for synthetic data:

\begin{enumerate}
	\item A training step expected to reach a satisfactory set of parameters for the encoder $h$ and the state model $g_\theta$.
	\item A finetuning step, during which only $W_y$, $b_y$ and $\Sigma_y$ are learned via Fisher's identity, for a set value of $\Sigma_x$.
\end{enumerate}

\clearpage
\section{Calibration on Oze data}
Calibration consists in fitting a metamodel, trained on simulated scenarios, to a specific building.
We aim at producing a model able to mimic this building's behavior.
In this study, we consider an additional objective: estimating the physical parameters $\lambda$ of the building.

Input:
\begin{itemize}
	\item A physical simulator simulating an equivalent simplified building model
	\item Building data for a month
	\item Ranges of physical parameters $\lambda$ for the building
	\item Ranges of usage settings $\psi$
\end{itemize}

Output:
\begin{itemize}
	\item A calibrated model
	\item Estimation of physical parameters $\lambda$
\end{itemize}

\subsection{Pretraining}
We sample a dataset of usage settings, physical parameters and weather scenarios from TRNSYS.
We fit a RNN on this dataset with a traditional gradient descent.

\subsection{Finetuning}
We explore the ranges of physical parameters and usage settings and adjust network weights.
We alternate iterations of the CMAES to explore the ranges with the finetuning procedure described in the previous section.

\begin{algorithm}
	\KwIn{ranges for $\lambda$ and $\psi_k$}
	\KwOut{$\hat \lambda$; calibrated model}
	\For{$i \gets 0$ \KwTo $T$}{
		$\hat \lambda \gets$ CMA-ES iterations\;
		$\theta, \Sigma_x, \Sigma_y \gets$ gradient descent on real data\;
	}
	\caption{Hybrid calibration procedure}
\end{algorithm}

Physical parameters will be submitted to energy managers to get an expert validation.

\subsection{Validation}
We apply the calibrated model on a set of fresh data.
For this step, we will allow the additional calibration of usage settings $\psi$ only.

\subsection{Optional: Optimization}
This subsection is optional as a successful optimization does not validate the calibration step.

\end{document}
