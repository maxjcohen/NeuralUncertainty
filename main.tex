\documentclass[11pt,a4paper]{report}

\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage[parfill]{parskip}

% For proofs
\usepackage{amsthm}
\newtheorem*{remark}{Remark}

\begin{document}
\title{Neural Uncertainty}
\author{Max Cohen}
\maketitle

\chapter{Model definition}
\section{Notations}
We consider the prediction task of a set of observations $(x^1, \cdots x^T)$ given a set of input $(u^1, \cdots u^T)$.

\section{Model}
We define a $L$ layer RNN followed by a fully connected layer. At time step $t$,

\begin{equation*}
    \left\{
    \begin{aligned}
        x_{t+1}   & = \tanh(W_x y_{t+1}^L + b_x)                                                               \\
        y_{t+1}^l & = \tanh(W_{yu}^l y^{l-1}_{t+1} + W_{yy}^l y^{l}_{t} + b_y^l) \quad \forall 1 \leq l \leq L \\
    \end{aligned}
    \right.
\end{equation*}

with $y_{t}^0 \equiv u_{t} \; \forall t$ and $y_{0}^l \equiv 0 \; \forall 1 \leq l \leq L$ .

Let's consider the weights of the last RNN and fully connected layers as $\Theta \equiv (W_{yu}^L, W_{yy}^L, b_y^L, W_x, b_x)$. We can define a new matrix $y_t$ at each time step corresponding to the concatenation of all RNN layers: $y_t \equiv (y_t^1 \cdots y_t^L)$. We also introduce two sequences of random noises as i.i.d real valued random variables $\epsilon$ and $\eta$. We can now write our model in terms of two functions $f$ and $g$ as:

\begin{equation}
    \left\{
    \begin{aligned}
        x_{t+1} & = f_\Theta(y_{t+1}) + \epsilon_{t+1}      & \text{observation model} \\
        y_{t+1} & = g_\Theta(y_{t}, u_{t+1}) + \eta_{t+1} & \text{state model}       \\
    \end{aligned}
    \right.
\end{equation}

\begin{align}
    Q(\hat \Theta_p, \Theta) & = \mathop{\mathbb{E}_{\hat \Theta_p}} \left[ \log \; p_{\Theta}(X_{1:T}, y_{1:T}, u_{1:T}) | y_{1:T} \right]
\end{align}

Let's develop the interior of expectation:

\begin{align*}
p_{\Theta}(X_{1:T}, y_{1:T}, u_{1:T}) & = \frac{1}{T} \log\left(\prod_{k=1}^{T} p_{\Theta}(y_k | y_{k-1}, u_k) p_{\Theta}(x_k | y_k)\right)                                                                \\
                             & = \frac{1}{T} \sum_{k=1}^{T} \log \; p_{\Theta}(y_k | y_{k-1}, u_k) + \log \; p_{\Theta}(x_k | y_k)                                                                  \\
                             & = \frac{1}{T} \sum_{k=1}^{T} \log \left(\det(2\pi\Sigma_y)^{-1/2} \exp(-\frac{1}{2}(y_k - g_\Theta(y_{k-1}, u_{k}))^T \Sigma_y^{-1} (y_k - g_\Theta(y_{k-1}, u_{k}))\right) \\
                             & + \log \left(det(2\pi\Sigma_x)^{-1/2} \exp(-\frac{1}{2}(x_k - f_\Theta(y_k))^T \Sigma_x^{-1} (x_k - f_\Theta(y_k))\right)                                                  \\
                             & = -\frac{1}{2} log |\Sigma_y| -\frac{1}{2} log |\Sigma_x|                                                                                                          \\
                             & - \frac{1}{2T} \sum_{k=1}^{T}(y_k - g_\Theta(y_{k-1}, u_{k}))^T \Sigma_y^{-1} (y_k - g_\Theta(y_{k-1}, u_{k}))                                                             \\
                             & - \frac{1}{2T} \sum_{k=1}^{T}(x_k - f_\Theta(y_k))^T \Sigma_x^{-1} (x_k - f_\Theta(y_k))                                                                                \\
\end{align*}
\end{document}