\documentclass[10pt,a4paper]{report}

\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage[parfill]{parskip}

% For proofs
\usepackage{amsthm}
\newtheorem*{remark}{Remark}

\begin{document}
\title{Neural Uncertainty}
\author{Max Cohen}
\maketitle

\chapter{Model definition}
\section{Notations}
We consider the prediction task of a set of observations $(y^1, \cdots y^T)$ given a set of input $(u^1, \cdots u^T)$.

\section{Model}
We define a $L$ layer RNN followed by a fully connected layer. At time step $t$,

\begin{equation*}
    \left\{
    \begin{aligned}
        y_{t+1}   & = \tanh(W_y x_{t+1}^L + b_y)                                                               \\
        x_{t+1}^l & = \tanh(W_{xx}^l x^{l}_{t} + W_{xu}^l x^{l-1}_{t+1} + b_x^l) \quad \forall 1 \leq l \leq L \\
    \end{aligned}
    \right.
\end{equation*}

with $x_{t}^0 \equiv u_{t} \; \forall t$ and $x_{0}^l \equiv 0 \; \forall 1 \leq l \leq L$ .

Let's consider the weights of the last RNN and fully connected layers as $\Theta \equiv (W_{xx}^L, W_{xu}^L, b_x^L, W_y, b_y)$. We can define a new matrix $y_t$ at each time step corresponding to the concatenation of all RNN layers: $x_t \equiv (x_t^1 \cdots x_t^L)$. We also introduce two sequences of random noises as i.i.d real valued random variables $\epsilon$ and $\eta$. We can now write our model in terms of two functions $f$ and $g$ as:

\begin{equation}
    \left\{
    \begin{aligned}
        y_{t+1} & = f_\Theta(x_{t+1}) + \epsilon_{t+1}    & \text{observation model} \\
        x_{t+1} & = g_\Theta(x_{t}, u_{t+1}) + \eta_{t+1} & \text{state model}       \\
    \end{aligned}
    \right.
\end{equation}

\begin{align}
    Q(\hat \Theta_p, \Theta) & = \mathop{\mathbb{E}_{\hat \Theta_p}} \left[ \log \; p_{\Theta}(X_{1:T}, y_{1:T}, u_{1:T}) | y_{1:T} \right]
\end{align}

Let's develop the interior of the expectation:

\begin{align*}
    p_{\Theta}(X_{1:T}, y_{1:T}, u_{1:T}) & = \frac{1}{T} \log\left(\prod_{k=1}^{T} p_{\Theta}(x_k | x_{k-1}, u_k) p_{\Theta}(y_k | x_k)\right)                                                                         \\
                                          & = \frac{1}{T} \sum_{k=1}^{T} \log \; p_{\Theta}(x_k | x_{k-1}, u_k) + \log \; p_{\Theta}(y_k | x_k)                                                                         \\
                                          & = \frac{1}{T} \sum_{k=1}^{T} \log \left(\det(2\pi\Sigma_x)^{-1/2} \exp(-\frac{1}{2}(x_k - g_\Theta(x_{k-1}, u_{k}))^T \Sigma_x^{-1} (x_k - g_\Theta(x_{k-1}, u_{k}))\right) \\
                                          & + \log \left(det(2\pi\Sigma_y)^{-1/2} \exp(-\frac{1}{2}(y_k - f_\Theta(x_k))^T \Sigma_y^{-1} (y_k - f_\Theta(x_k))\right)                                                   \\
                                          & = -\frac{1}{2} \log|\Sigma_x| -\frac{1}{2} \log|\Sigma_y|                                                                                                                   \\
                                          & - \frac{1}{2T} \sum_{k=1}^{T}(x_k - g_\Theta(x_{k-1}, u_{k}))^T \Sigma_x^{-1} (x_k - g_\Theta(x_{k-1}, u_{k}))                                                              \\
                                          & - \frac{1}{2T} \sum_{k=1}^{T}(y_k - f_\Theta(x_k))^T \Sigma_y^{-1} (y_k - f_\Theta(x_k))                                                                                    \\
\end{align*}

In order to find the minimal value of $\Sigma_x$, we can search for the zeros of the derivate of the concave function $\Sigma_x \mapsto p_{\Theta}(X_{1:T}, y_{1:T}, u_{1:T})$.

\begin{align*}
    \frac{\partial p_{\Theta}(X_{1:T}, y_{1:T}, u_{1:T})}{\partial \Sigma_x^{-1}} & = \frac{1}{2} \Sigma_x - \frac{1}{2T} \sum_{k=1}^T 2 \Sigma_x^{-1} (x_k - f_{\Theta}(x_{k-1}, u_k)) \\
                                                                                  & = \frac{1}{2} \Sigma_x - \frac{\Sigma_x^{-1}}{T} \sum_{k=1}^T x_k - f_{\Theta}(x_{k-1}, u_k)
\end{align*}

\begin{align*}
    \frac{\partial p_{\Theta}(X_{1:T}, y_{1:T}, u_{1:T})}{\partial \Sigma_x^{-1}} = 0 \implies diag(\Sigma_x) = \sum_{k=1}^T x_k - f_{\Theta}(x_{k-1}, u_k)
\end{align*}
\end{document}