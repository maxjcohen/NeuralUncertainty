\documentclass[11pt,a4paper]{report}

\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage[parfill]{parskip}

% For proofs
\usepackage{amsthm}
\newtheorem*{remark}{Remark}

\begin{document}
\title{Neural Uncertainty}
\author{Max Cohen}
\maketitle

\chapter{Model definition}
\section{Notations}
We consider the prediction task of a hidden state $(x^1, \cdots x^T)$ given a set of observations $(y^1, \cdots y^T)$.

\section{Model}
We define a $L$ layer RNN followed by a fully connected layer. At time step $t$,

\begin{equation*}
    \left\{
    \begin{aligned}
        x_{t+1}   & = \tanh(W_x h_{t+1}^L + b_x)                                                               \\
        h_{t+1}^l & = \tanh(W_{hy}^l h^{l-1}_{t+1} + W_{hh}^l h^{l}_{t} + b_h^l) \quad \forall 1 \leq l \leq L \\
    \end{aligned}
    \right.
\end{equation*}

with $h_{t}^0 \equiv y_{t} \; \forall t$ and $h_{0}^l \equiv 0 \; \forall 1 \leq l \leq L$ .

Let's consider the weights of the last RNN and fully connected layers as $\Theta \equiv (W_{hy}^L, W_{hh}^L, b_h^L, W_x, b_x)$. We can define a new matrix $h_t$ at each time step corresponding to the concatenation of all RNN layers: $h_t \equiv (h_t^1 \cdots h_t^L)$. We also introduce two sequences of random noises as i.i.d real valued random variables $\epsilon$ and $\eta$. We can now write our model in terms of two functions $f$ and $g$ as:

\begin{equation}
    \left\{
    \begin{aligned}
        x_{t+1} & = f_\Theta(h_t) + \epsilon_t      & \text{observation model} \\
        h_{t+1} & = g_\Theta(h_t, y_{t+1}) + \eta_t & \text{state model}       \\
    \end{aligned}
    \right.
\end{equation}

\begin{align*}
    Q(\hat \Theta_p, \Theta) & = \mathop{\mathbb{E}_{\hat \Theta_p}} \left[ \log \; p_{\Theta}(X_{1:T}, h_{1:T}, y_{1:T}) | X_{1:T} \right]                                                          \\
                             & \simeq \frac{1}{T} \sum_{i=1}^T  log \; p_{\Theta}(X_{1:T}, h_{1:T}, y_{1:T})                                                                                         \\
                             & = \frac{1}{T} \log\left(\prod_{k=1}^{T} p_{\Theta}(h_k | h_{k-1}, y_k) p_{\Theta}(x_k | h_k)\right)                                                                   \\
                             & = \frac{1}{T} \sum_{k=1}^{T} \log \; p_{\Theta}(h_k | h_{k-1}, y_k) + \log \; p_{\Theta}(x_k | h_k)                                                                   \\
                             & = \frac{1}{T} \sum_{k=1}^{T} \log \left[det(2\pi\Sigma_h)^{-1/2} \exp(-\frac{1}{2}(h_k - g_\Theta(h_{k-1}, y_{k}))^T \Sigma_h (h_k - g_\Theta(h_{k-1}, y_{k}))\right] \\
                             & + \log \left[det(2\pi\Sigma_x)^{-1/2} \exp(-\frac{1}{2}(x_k - f_\Theta(h_k))^T \Sigma_h (x_k - f_\Theta(h_k))\right]                                                  \\
                             & = -\frac{1}{2} log |\Sigma_h| -\frac{1}{2} log |\Sigma_x|                                                                                                             \\
                             & - \frac{1}{2T} \sum_{k=1}^{T}(h_k - g_\Theta(h_{k-1}, y_{k}))^T \Sigma_h (h_k - g_\Theta(h_{k-1}, y_{k}))                                                             \\
                             & - \frac{1}{2T} \sum_{k=1}^{T}(x_k - f_\Theta(h_k))^T \Sigma_x (x_k - f_\Theta(h_k))                                                                                   \\
\end{align*}
\end{document}